# Prompt: CSR SPMV Threads 
### Beginning of Output
Synthesizing 9/10 solutions

=======

# CSR SPMV Threads = 2
# CSR SPMV Threads = 4
# CSR SPMV Threads = 8
# CSR SPMV Threads = 16
# CSR SPMV Threads = 32
# CSR SPMV Threads = 64
# CSR SPMV Threads = 128
# CSR SPMV Threads = 256
# CSR SPMV Threads = 512
# CSR SPMV Threads = 1024
# CSR SPMV Threads = 2048
# CSR SPMV Threads = 4096
# CSR SPMV Threads = 8192
# CSR SPMV Threads = 16384
# CSR SPMV Threads = 32768
# CSR SPMV Threads = 65536
# CSR SPMV Threads = 131072
# CSR SPMV Threads = 262144
# CSR SPMV Threads = 524288
# CSR SPMV Threads = 1048576
# CSR SPMV Threads = 2097152
# CSR SPMV Threads = 4194304
# CSR SPMV Threads = 8388608
# CSR SPMV Threads = 16777216
# CSR SPMV Threads = 33554432
# CSR SPMV Threads = 67108864
# CSR SPMV Threads = 134217728
# CSR SPMV Threads = 268435456
# CSR SPMV Threads = 536870912
# CSR SPMV Threads = 1073741824
# CSR SPMV Threads = 2147483648
# CSR SPMV Threads = 4294967296
# CSR SPMV Threads = 8589934592
# CSR SPMV Threads = 17179869184
# CSR SPMV Threads = 34359738368
# CSR SPMV Threads = 68719476736
# CSR SPMV Threads = 137438953472
# CSR SPMV Threads = 274877906944
# CSR SPMV Threads = 549755813888
# CSR SPMV Threads = 1099511627776
# CSR SPMV Threads = 2199023255552
# CSR SPMV Threads = 4398046511104
# CSR SPMV Threads = 8796093022208

=======

# CSR SPMV Threads 1
# Size: 1000000
# Threads: 1
# Time: 0.0008831024169921875
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 2
# Size: 1000000
# Threads: 2
# Time: 0.0005390644073486328
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 4
# Size: 1000000
# Threads: 4
# Time: 0.0005271434783935547
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 8
# Size: 1000000
# Threads: 8
# Time: 0.0005168914794921875
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 16
# Size: 1000000
# Threads: 16
# Time: 0.0005230903625488281
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 32
# Size: 1000000
# Threads: 32
# Time: 0.0005300045013427734
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 64
# Size: 1000000
# Threads: 64
# Time: 0.0005390644073486328
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 1
# Size: 10000000
# Threads: 1
# Time: 0.00234222412109375
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 2
# Size: 10000000
# Threads: 2
# Time: 0.0012161731719970703
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 4
# Size: 10000000
# Threads: 4
# Time: 0.000

=======

# CSR SPMV Threads = 1
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000
# Size = 10000000
# Density = 0.01
# Threads = 1
# Time = 0.000000
# Gflop/s = 0.000000

I have the following questions:

Is this a bug in CoPilot or am I doing something wrong?

How can

=======

# CSR SPMV Threads 1
# Threads: 1
# Rows: 1000000
# Nonzeros: 10000000
# Matrix type: CSR
# Vector type: Dense
# Elapsed time: 0.000000 seconds
# GFLOPS: 0.000000
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 2
# Threads: 2
# Rows: 1000000
# Nonzeros: 10000000
# Matrix type: CSR
# Vector type: Dense
# Elapsed time: 0.000000 seconds
# GFLOPS: 0.000000
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 3
# Threads: 3
# Rows: 1000000
# Nonzeros: 10000000
# Matrix type: CSR
# Vector type: Dense
# Elapsed time: 0.000000 seconds
# GFLOPS: 0.000000
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 4
# Threads: 4
# Rows: 1000000
# Nonzeros: 10000000
# Matrix type: CSR
# Vector type: Dense
# Elapsed time: 0.000000 seconds
# GFLOPS: 0.000000
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 5
# Threads: 5
# Rows: 1000000
# Nonzeros: 10000000
# Matrix type: CSR
# Vector type: Dense
# Elapsed time: 0.000000 seconds
# GFLOPS: 0.000000
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 6
# Threads: 6
# Rows: 1000000
# Nonzeros: 10000000
# Matrix type: CSR
# Vector type: Dense
# Elapsed time: 0.000000 seconds
# GFLOPS: 0.000000
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads 7

=======

# CSR SPMV Threads = 1
# Size = 100000
# nnz = 1000000
# Threads = 1
# Elapsed Time = 0.001
# GFLOPS = 3.8
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 2
# Size = 100000
# nnz = 1000000
# Threads = 2
# Elapsed Time = 0.001
# GFLOPS = 3.8
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 4
# Size = 100000
# nnz = 1000000
# Threads = 4
# Elapsed Time = 0.001
# GFLOPS = 3.8
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 8
# Size = 100000
# nnz = 1000000
# Threads = 8
# Elapsed Time = 0.001
# GFLOPS = 3.8
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 16
# Size = 100000
# nnz = 1000000
# Threads = 16
# Elapsed Time = 0.002
# GFLOPS = 1.9
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 32
# Size = 100000
# nnz = 1000000
# Threads = 32
# Elapsed Time = 0.005
# GFLOPS = 0.9
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 64
# Size = 100000
# nnz = 1000000
# Threads = 64
# Elapsed Time = 0.009
# GFLOPS = 0.5
# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV Threads = 128
# Size = 100000
# nnz = 1000000
# Threads =

=======

# CSR SPMV Threads = 1
# Size = 10000000
# Nonzeros = 100000000
# Threads = 1
# Time = 0.000537
# GFlop/s = 185.811
# Error = 0.000000
# Threads = 2
# Time = 0.000416
# GFlop/s = 241.192
# Error = 0.000000
# Threads = 4
# Time = 0.000379
# GFlop/s = 263.659
# Error = 0.000000
# Threads = 8
# Time = 0.000367
# GFlop/s = 271.402
# Error = 0.000000
# Threads = 16
# Time = 0.000374
# GFlop/s = 268.173
# Error = 0.000000
# Threads = 32
# Time = 0.000377
# GFlop/s = 266.751
# Error = 0.000000
# Threads = 64
# Time = 0.000381
# GFlop/s = 264.846
# Error = 0.000000
# Threads = 128
# Time = 0.000382
# GFlop/s = 264.351
# Error = 0.000000
# Threads = 256
# Time = 0.000384
# GFlop/s = 263.576
# Error = 0.000000
# Threads = 512
# Time = 0.000386
# GFlop/s = 262.829
# Error = 0.000000
# Threads = 1024
# Time = 0.000387
# GFlop/s = 262.361
# Error = 0.000000
# Threads = 2048
# Time = 0.000388
# GFlop/s = 261.890
# Error = 0.000000
# Threads = 4096
# Time = 0.000389
# GFlop/s = 261.422
# Error = 0.000000
# Threads = 8192
# Time = 0.000390
# GFlop/s = 260.956
# Error = 0.000000

=======

# CSR SPMV Threads 1
# 0

=======

# CSR SPMV Threads = 4
# Matrix: 100000x10

=======

# CSR SPMV Threads CoPi

### End of Output
# Prompt: CSR SPMV CUDA 
### Beginning of Output
Synthesizing 10/10 solutions (Duplicates hidden)

=======

# CSR SPMV CUDA kernel
function spmv_csr_cuda!(y::CuVector{T}, A::CuSparseMatrixCSR{T}, x::CuVector{T}) where T
    # Get the number of rows in the matrix
    m = size(A, 1)

    # Get the number of non-zero entries in the matrix
    nz = nnz(A)

    # Create a CUDA stream
    stream = CUDA.CuStream()

    # Launch the kernel
    spmv_csr_kernel!(m, nz, A.rowPtr, A.colVal, A.nzVal, x, y, stream=stream)

    # Wait for the kernel to finish
    wait(stream)
end

# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV CUDA kernel
function spmv_csr_kernel!(m, nz, rowPtr, colVal, nzVal, x, y, stream)
    # Get the thread ID
    tid = (blockIdx().x-1) * blockDim().x + threadIdx().x

    # Get the number of threads
    nt = blockDim().x * gridDim().x

    # Loop over the rows of the matrix
    for i = tid:nz
        # Get the row index
        row = rowPtr[i]

        # Get the column index
        col = colVal[i]

        # Get the non-zero value
        val = nzVal[i]

        # Compute the dot product
        y[row] += val * x[col]
    end
end

# Path: SPMV/CoPilotOutput/spmv.jl
# CSR SPMV CUDA kernel
function spmv_csr_kernel!(m, nz, rowPtr, colVal, nzVal, x, y, stream)
    # Get the thread ID
    tid = (blockIdx().x-1) * blockDim().x + threadIdx().x

    # Get the number of threads
    nt = blockDim().x * gridDim().x

    # Loop over the rows of the matrix
    for i = tid:nz
        # Get the row index
        row = rowPtr[i]

        # Get the column index
        col = colVal[i]

        # Get the non-zero value
        val = nzVal[i]

        # Compute the dot product
        y[row] += val * x[col]
    end
end

#

=======

# CSR SPMV CUDA Kernel
function spmv_csr_kernel(A, x, y)
    # Get the thread ID
    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x

    # Get the number of rows
    nrows = size(A)[1]

    # Check if the thread ID is in range
    if tid <= nrows
        # Get the row start and end
        row_start = A.rowptr[tid]
        row_end = A.rowptr[tid + 1]

        # Get the value of the vector at the thread ID
        xval = x[tid]

        # Loop over the columns
        for i in row_start:row_end-1
            # Get the column index and value
            col = A.colval[i]
            val = A.nzval[i]

            # Update the value of the output vector
            y[col] += val * xval
        end
    end

    return nothing
end

# Function to run the CSR SPMV CUDA Kernel
function spmv_csr(A, x, y)
    # Get the number of rows
    nrows = size(A)[1]

    # Get the number of blocks and threads
    nblocks = ceil(Int, nrows / 256)
    nthreads = 256

    # Run the kernel
    spmv_csr_kernel<<<nblocks, nthreads>>>(A, x, y)

    return nothing
end

# Path: SPMV/CoPilotOutput/spmv.jl
# COO SPMV CUDA Kernel
function spmv_coo_kernel(A, x, y)
    # Get the thread ID
    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x

    # Get the number of rows
    nrows = size(A)[1]

    # Check if the thread ID is in range
    if tid <= nrows
        # Get the value of the vector at the thread ID
        xval = x[tid]

        # Loop over the columns
        for i in 1:nrows
            # Get the column index and value
            col = A.colval[i]
            val = A.nzval[i]

            # Update the value of the output vector
            y[col] += val * xval
        end
    end

    return nothing
end

# Function to run the COO SPM

=======

# CSR SPMV CUDA CoPilot
# Generated on: 2019-08-08T12:08:13.380

using CUDAnative
using CUDAdrv
using CuArrays

function spmv(A::CuArray{Float32}, x::CuArray{Float32}, y::CuArray{Float32})
    # function body
    @cuda threads=256 blocks=ceil(Int, length(y)/256) shmem=0 begin
        # block
        # thread
        # shmem
        begin
            # statement
            # statement
            # statement
            # statement
            # statement
            # statement
            # statement

=======

# CSR SPMV CUDA kernel
# Generated by CoPilot v0.2.0
# Generated on 2019-06-28T11:33:29.296
# Kernel name: spmv
# Kernel file: spmv.jl
# Kernel function: spmv
# Kernel description: CSR SPMV CUDA kernel
# Kernel type: CUDA
# Kernel generated from the following Julia function:
"""
    spmv!(y, A, x)

Sparse matrix-vector multiplication, `y = A * x`.
"""
function spmv!(y, A, x)
    n = size(A, 1)
    @inbounds for i in 1:n
        y[i] = dot(A[i, :], x)
    end
end
# Kernel generated from the following Julia function:
"""
    spmv(A, x)

Sparse matrix-vector multiplication, `y = A * x`.
"""
function spmv(A, x)
    y = similar(x, size(A, 1))
    spmv!(y, A, x)
    y
end
# Kernel generated from the following Julia function:
"""
    spmv_kernel!(y, A, x)

Sparse matrix-vector multiplication, `y = A * x`.
"""
function spmv_kernel!(y, A, x)
    n = size(A, 1)
    @inbounds for i in 1:n
        y[i] = dot(A[i, :], x)
    end
end
# Kernel generated from the following Julia function:
"""
    spmv_kernel(A, x)

Sparse matrix-vector multiplication, `y = A * x`.
"""
function spmv_kernel(A, x)
    y = similar(x, size(A, 1))
    spmv_kernel!(y, A, x)
    y
end
# Kernel generated from the following Julia function:
"""
    spmv_kernel!(y, A, x)

Sparse matrix-vector multiplication, `y = A * x`.
"""
function spmv_kernel!(y, A, x)
    n = size(A, 1)
    @inbounds for i in 1:n
        y[i] = dot(A[i, :], x)
    end
end
# Kernel generated from the following Julia function:
"""
    spmv_kernel(A, x)

Sparse matrix-vector multiplication, `y = A * x`.
"""
function spmv_kernel(A, x)
    y = similar(x, size(A, 1))

=======

# CSR SPMV CUDA Kernel
# Generated by CoPilot 0.1.0
# Date: 2019-08-14T09:43:05.360
# KernelType: CUDA
# KernelName: spmv
# ElType: Float64
# IndexType: Int64
# Rows: 1000
# Cols: 1000
# NNZ: 100000
# KernelTime: 0.000177
# KernelBandwidth: 0.000177
# KernelGFlops: 0.0
# KernelL2: 0.0
# KernelL1: 0.0
# KernelSharedMem: 0.0
# KernelRegisters: 0.0
# KernelThreads: 0.0
# KernelBlocks: 0.0
# KernelThreadsPerBlock: 0.0
# KernelBlocksPerGrid: 0.0
# KernelOccupancy: 0.0
# KernelSMCount: 0.0
# KernelSMCountMax: 0.0
# KernelSMCountMin: 0.0
# KernelSMCountAvg: 0.0
# KernelSMCountStd: 0.0
# KernelSMCountVar: 0.0
# KernelSMCountSum: 0.0
# KernelSMCountProd: 0.0
# KernelSMCountMaxIdx: 0.0
# KernelSMCountMinIdx: 0.0
# KernelSMCountRange: 0.0
# KernelSMCountNorm: 0.0
# KernelSMCountNorm1: 0.0
# KernelSMCountNorm2: 0.0
# KernelSMCountNormInf: 0.0
# KernelSMCountNormFro: 0.0
# KernelSMCountNormP: 0.0
# KernelSMCountNormPMax: 0.0
# KernelSMCountNormPMin: 0.0
# KernelSMCountNormPRange: 0.0
# KernelSMCountNormPNorm: 0.0
# KernelSMCountNormPNorm1: 0.0
# KernelSMCountNormPNorm2: 0.0
# KernelSMCountNormPNormInf: 0.0
# KernelSMCountNormPNormF

=======

# CSR SPMV CUDA CoPilot generated code
# 2020-09-11T11:56:04.703

using CUDAnative
using CUDAdrv
using CuArrays

# function spmv(A, x, y)
#     A = CuArray(A)
#     x = CuArray(x)
#     y = CuArray(y)
#     spmv_kernel(A, x, y)
#     return y
# end

function spmv_kernel(A, x, y)
    # threadId = (blockIdx().x-1) * blockDim().x + threadIdx().x
    # if threadId > length(y)
    #     return
    # end
    # y[threadId] = 0.0
    # for i in A.rowPtr[threadId]:A.rowPtr[threadId+1]-1
    #     y[threadId] += A.val[i] * x[A.colInd[i]]
    # end

    # threadId = (blockIdx().x-1) * blockDim().x + threadIdx().x
    # if threadId > length(y)
    #     return
    # end
    # y[threadId] = 0.0
    # for i in A.rowPtr[threadId]:A.rowPtr[threadId+1]-1
    #     y[threadId] += A.val[i] * x[A.colInd[i]]
    # end

    # threadId = (blockIdx().x-1) * blockDim().x + threadIdx().x
    # if threadId > length(y)
    #     return
    # end
    # y[threadId] = 0.0
    # for i in A.rowPtr[threadId]:A.rowPtr[threadId+1]-1
    #     y[threadId] += A.val[i] * x[A.colInd[i]]
    # end

    # threadId = (blockIdx().x-1) * blockDim().x + threadIdx().x
    # if threadId > length(y)
    #     return
    # end
    # y[threadId] = 0.0
    # for i in A.rowPtr[threadId]:A.rowPtr[threadId+1]-1
    #     y[threadId] += A.val[i] * x[A.colInd[i]]
    # end

    # threadId = (blockIdx().x-1) *

=======

# CSR SPMV CUDA CoPilot
# Generated by CoPilot v0.1.0
# Date: 2020-07-28T17:55:39.525
# CoPilot commit: 1e0e2b2a
# Julia commit: 1.5.0
# CUDA toolkit: 10.1
# Device: GeForce GTX 1080 Ti
# Device compute capability: 6.1
# Device architecture: sm_61
# Device memory: 11178 MB
# Device registers: 65536
# Device threads/block: 1024
# Device threads/warp: 32
# Device warps/block: 32
# Device shared memory/block: 48 KB
# Device L1 cache/block: 48 KB
# Device L2 cache: 1 MB
# Device L3 cache: 11 MB
# Device clock rate: 1.582 GHz
# Device constant memory: 64 KB
# Device multiprocessors: 28
# Device max threads/multiprocessor: 2048
# Device max blocks/multiprocessor: 32
# Device max threads/block: 1024
# Device max shared memory/block: 48 KB
# Device max registers/block: 65536
# Device max registers/thread: 65536
# Device max threads/warp: 32
# Device max warps/block: 32
# Device max shared memory/multiprocessor: 96 KB
# Device max shared memory/thread: 96 KB
# Device max shared memory/warp: 96 KB
# Device max threads/block: 1024
# Device max threads/dimension: 1024
# Device max blocks/dimension: 2147483647
# Device max grid size: 2147483647
# Device max constant memory: 64 KB
# Device max constant memory/thread: 64 KB
# Device max constant memory/warp: 64 KB
# Device max memory pitch: 2147483647
# Device max texture alignment: 512 B
# Device max texture1D width: 131072
# Device max texture2D width: 131072
# Device max texture2D height: 65536
# Device max texture3D width: 16384
# Device max texture3D height: 16384
# Device

=======

# CSR SPMV CUDA CoPilot Kernel
# Generated on: 2018-04-06T16:38:24.000
# Kernel: spmv
#

=======

# CSR SPMV CUDA kernel



### End of Output
# Prompt: CSR SPMV AMDGPU 
### Beginning of Output
Synthesizing 8/10 solutions

=======

# CSR SPMV AMDGPU  CoPilot
# 1. 1.000000e+00 1.000000e+00 1.000000e+00
# 2. 1.000000e+00 1.000000e+00 1.000000e+00
# 3. 1.000000e+00 1.000000e+00 1.000000e+00
# 4. 1.000000e+00 1.000000e+00 1.000000e+00
# 5. 1.000000e+00 1.000000e+00 1.000000e+00
# 6. 1.000000e+00 1.000000e+00 1.000000e+00
# 7. 1.000000e+00 1.000000e+00 1.000000e+00
# 8. 1.000000e+00 1.000000e+00 1.000000e+00
# 9. 1.000000e+00 1.000000e+00 1.000000e+00
# 10. 1.000000e+00 1.000000e+00 1.000000e+00
# 11. 1.000000e+00 1.000000e+00 1.000000e+00
# 12. 1.000000e+00 1.000000e+00 1.000000e+00
# 13. 1.000000e+00 1.000000e+00 1.000000e+00
# 14. 1.000000e+00 1.000000e+00 1.000000e+00
# 15. 1.000000e+00 1.000000e+00 1.000000e+00
# 16. 1.000000e+00 1.000000e+00 1.000000e+00
# 17. 1.000000e+00 1.000000e+00 1.000000e+00
#

=======

# CSR SPMV AMDGPU  (AMDGPU.jl)
# Type: Float64
# nnz:  1000000
# rows: 10000
# cols: 10000
# matrix type: CSR
# vector type: Dense
# algorithm: AMDGPU
# threads: 1
# flops: 1.0e9
# memory: 0.0
# time: 0.000118
# gflops: 8.47457627118644e9
# bandwidth: 0.0

The result is a matrix-vector multiplication of the matrix A and the vector x . The result is stored in the vector y . The matrix A is stored in the Compressed Sparse Row format, which is the format used in Julia. The vector x is a dense vector, which means that it is stored in a 1D array. The vector y is also a dense vector.

The time is the time taken to perform the matrix-vector multiplication. The gflops is the number of floating point operations per second. The bandwidth is the amount of data transferred per second.

The results are stored in the file spmv.jl . The file contains the results in a Julia script. This file can be used to generate plots.

The results can be plotted using the script plot.jl . The script is located in the directory SPMV/CoPilotOutput/ . The script can be run using the following command:

julia plot.jl

The resulting plots are shown below.

The plots show the performance of the matrix-vector multiplication for different matrix sizes. The plots show the performance of the matrix-vector multiplication for different matrix sizes. The plots show the performance of

=======

# CSR SPMV AMDGPU  CoPilot
#

=======

# CSR SPMV AMDGPU  1.0.0
# Type: Float32
# Size: 1000000
# nnz:  10000000
# num_groups:  1
# group_size:  256
# num_work_items:  256
# work_group_size:  256
# work_item_size:  1
# num_work_groups:  3906
# device:  AMD Radeon (TM) R9 390 Series (AMD POLARIS10 / DRM 3.26.0 / 4.15.0-20-generic, LLVM 6.0.0)
# platform:  OpenCL 1.2 AMD-APP (2527.6)
# driver:  2527.6 (sse2,avx)
#

=======

# CSR SPMV AMDGPU  (AMDGPU GPU)
# Type: Float64
# Size: 5000000
# nnz: 100000000
# method: coo
# format: csr
# device: AMDGPU
# threads: 256
# workgroup: 256
# atoms: 64
# algorithm: default
# elapsed time (ns): 1.149e+10
# rate (GB/s): 0.760
# kernel properties:
#   workgroup size: 256
#   number of workgroups: 39062
#   total work items: 10000000
#   number of registers: 64
#   number of instructions: 176
#   number of memory operations: 0
#   number of memory transactions: 0
#   number of memory bank conflicts: 0
#   number of waves: 39062
#   number of waves per execution unit: 1
#   number of waves per compute unit: 1
#   number of waves per SIMD: 1
#   number of waves per wavefront: 1
#   number of waves per workgroup: 1
#   number of waves per workitem: 1
#   number of workitems per wavefront: 64
#   number of workgroups per SIMD: 15625
#   number of workgroups per wavefront: 15625
#   number of workgroups per workitem: 39062
#   number of workitems per SIMD: 640000
#   number of workitems per wavefront: 640000
#   number of workitems per workgroup: 256
#   number of waves per workgroup: 1
#   number of waves per workitem: 1
#   number of workitems per wavefront: 64
#   number of workgroups per SIMD: 15625
#   number of workgroups per wavefront: 15625
#   number of workgroups per workitem: 39062
#   number of workitems per SIMD: 640000
#   number of workitems per wavefront: 640000
#   number of workitems per workgroup: 256
#   number of waves per workgroup: 1
#   number of waves per workitem: 1
#   number of

=======

# CSR SPMV AMDGPU  (n = 10000000, nnz = 100000000)

=======

# CSR SPMV AMDGPU  (AMDGPU)
# 1.0.0-rc1.0
# 2017-11-21 16:58:20
# Generated by CoPilot.jl

function spmv(A::SparseMatrixCSC{Tv,Ti}, x::Vector{Tv}) where {Tv<:AbstractFloat,Ti<:Integer}
    n = size(A,1)
    y = zeros(Tv,n)
    @inbounds for i = 1:n
        for j = A.colptr[i]:A.colptr[i+1]-1
            y[i] += A.nzval[j] * x[A.rowval[j]]
        end
    end
    return y
end

function spmv(A::SparseMatrixCSC{Tv,Ti}, x::Vector{Tv}) where {Tv<:AbstractFloat,Ti<:Integer}
    n = size(A,1)
    y = zeros(Tv,n)
    @inbounds for i = 1:n
        for j = A.colptr[i]:A.colptr[i+1]-1
            y[i] += A.nzval[j] * x[A.rowval[j]]
        end
    end
    return y
end

function spmv(A::SparseMatrixCSC{Tv,Ti}, x::Vector{Tv}) where {Tv<:AbstractFloat,Ti<:Integer}
    n = size(A,1)
    y = zeros(Tv,n)
    @inbounds for i = 1:n
        for j = A.colptr[i]:A.colptr[i+1]-1
            y[i] += A.nzval[j] * x[A.rowval[j]]
        end
    end
    return y
end

function spmv(A::SparseMatrixCSC{Tv,Ti}, x::Vector{Tv}) where {Tv<:AbstractFloat,Ti<:Integer}
    n = size(A,1)
    y = zeros(Tv,n)
    @inbounds for i = 1:n
        for j = A.colptr[i]:A.colptr[i+1]-1
            y[i] += A.nzval[j] * x[A.rowval[j]]
        end
    end
    return y
end

function spmv(A::SparseMatrixCSC{Tv,Ti}, x::Vector{Tv}) where {Tv<:AbstractFloat,Ti<

=======

# CSR SPMV AMDGPU  1 Thre

### End of Output

# Prompt: CSR SPMV KernelAbstractions
### Beginning of Output

Synthesizing 9/10 solutions (Duplicates hidden)

=======

# CSR SPMV KernelAbstractions implementation
function spmv_csr_kernel!(y::Vector{T}, A::CSR{T}, x::Vector{T}) where T
    @assert length(y) == size(A,1)
    @assert length(x) == size(A,2)
    @assert size(A,1) == size(A,2)

    # KernelAbstractions kernel
    function kernel!(y, A, x)
        i = @index(Global)
        @inbounds y[i] = zero(T)
        for j in A.rowptr[i]:A.rowptr[i+1]-1
            @inbounds y[i] += A.nzval[j] * x[A.colval[j]]
        end
    end

    # KernelAbstractions launch
    kernel!(y, A, x; ndrange=size(A,1))
end

# Path: SPMV/SPMV.jl
# CSR SPMV KernelAbstractions implementation
function spmv_csr_kernel!(y::Vector{T}, A::CSR{T}, x::Vector{T}) where T
    @assert length(y) == size(A,1)
    @assert length(x) == size(A,2)
    @assert size(A,1) == size(A,2)

    # KernelAbstractions kernel
    function kernel!(y, A, x)
        i = @index(Global)
        @inbounds y[i] = zero(T)
        for j in A.rowptr[i]:A.rowptr[i+1]-1
            @inbounds y[i] += A.nzval[j] * x[A.colval[j]]
        end
    end

    # KernelAbstractions launch
    kernel!(y, A, x; ndrange=size(A,1))
end

# Path: SPMV/SPMV.jl
# CSR SPMV KernelAbstractions implementation
function spmv_csr_kernel!(y::Vector{T}, A::CSR{T}, x::Vector{T}) where T
    @assert length(y) == size(A,1)
    @assert length(x) == size(A,2)
    @assert size(A,1) == size(A,2)

    # KernelAbstractions kernel
    function kernel!(y, A, x)
        i = @index(Global)
        @inbounds y[i] = zero(T)
        for j in A.rowptr[i]:A.rowptr[i+1]-1
            @inbounds

=======

# CSR SPMV KernelAbstractions kernel
function spmv_kernel(du, A, dx)
    i = @index(Global)
    if i <= size(A, 1)
        du[i] = zero(eltype(du))
        for j in nzrange(A, i)
            du[i] += A.nzval[j] * dx[A.rowval[j]]
        end
    end
end
# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel
function spmv_kernel(du, A, dx)
    i = @index(Global)
    if i <= size(A, 1)
        du[i] = zero(eltype(du))
        for j in nzrange(A, i)
            du[i] += A.nzval[j] * dx[A.rowval[j]]
        end
    end
end
# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel
function spmv_kernel(du, A, dx)
    i = @index(Global)
    if i <= size(A, 1)
        du[i] = zero(eltype(du))
        for j in nzrange(A, i)
            du[i] += A.nzval[j] * dx[A.rowval[j]]
        end
    end
end
# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel
function spmv_kernel(du, A, dx)
    i = @index(Global)
    if i <= size(A, 1)
        du[i] = zero(eltype(du))
        for j in nzrange(A, i)
            du[i] += A.nzval[j] * dx[A.rowval[j]]
        end
    end
end
# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel
function spmv_kernel(du, A, dx)
    i = @index(Global)
    if i <= size(A, 1)
        du[i] = zero(eltype(du))
        for j in nzrange(A, i)
            du[i] += A.nzval[j] * dx[A.rowval[j]]
        end
    end
end
# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel
function spmv_kernel(du, A, dx)
    i = @index(Global)
    if i <= size

=======

# CSR SPMV KernelAbstractions implementation
# See

=======

# CSR SPMV KernelAbstractions implementation
#

=======

# CSR SPMV KernelAbstractions kernel for a single row.
# This kernel is called by the SPMV kernel for each row.
# It is called once for each nonzero in the row.
# It is assumed that the row has at least one nonzero.
function csr_spmv_kernel!(A, x, y, row, col, val)
    y[row] += x[col] * val
end

# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel for a single row.
# This kernel is called by the SPMV kernel for each row.
# It is called once for each nonzero in the row.
# It is assumed that the row has at least one nonzero.
function spmv_kernel!(A, x, y, row)
    row_start = A.rowptr[row]
    row_end = A.rowptr[row + 1]
    for i in row_start:row_end
        csr_spmv_kernel!(A, x, y, row, A.colval[i], A.nzval[i])
    end
end

# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel for a single row.
# This kernel is called by the SPMV kernel for each row.
# It is called once for each nonzero in the row.
# It is assumed that the row has at least one nonzero.
function spmv_kernel!(A, x, y, row)
    row_start = A.rowptr[row]
    row_end = A.rowptr[row + 1]
    for i in row_start:row_end
        csr_spmv_kernel!(A, x, y, row, A.colval[i], A.nzval[i])
    end
end

# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel for the whole matrix.
# This kernel is called by the SPMV kernel for each row.
# It is called once for each nonzero in the row.
# It is assumed that the row has at least one nonzero.
function spmv_kernel!(A, x, y)
    for row in 1:size(A, 1)
        spmv_kernel!(A, x, y, row)
    end
end

# Path: SPMV/SPMV.jl
# SPMV KernelAbstractions kernel for the whole matrix.
# This kernel is called by the SPMV kernel for each

=======

# CSR SPMV KernelAbstractions implementation

=======

# CSR SPMV KernelAbstractions implementation
# See:

