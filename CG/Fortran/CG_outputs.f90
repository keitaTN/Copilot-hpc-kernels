! Conjugate Gradients OpenMP

Synthesizing 9/10 solutions (Duplicates hidden)

=======

! Conjugate Gradients OpenMP 1.0

! Conjugate Gradients OpenMP 2.0

! Conjugate Gradients OpenMP 3.0

! Conjugate Gradients OpenMP 4.0

! Conjugate Gradients OpenMP 5.0

! Conjugate Gradients OpenMP 6.0

! Conjugate Gradients OpenMP 7.0

! Conjugate Gradients OpenMP 8.0

! Conjugate Gradients OpenMP 9.0

! Conjugate Gradients OpenMP 10.0

! Conjugate Gradients OpenMP 11.0

! Conjugate Gradients OpenMP 12.0

! Conjugate Gradients OpenMP 13.0

! Conjugate Gradients OpenMP 14.0

! Conjugate Gradients OpenMP 15.0

! Conjugate Gradients OpenMP 16.0

! Conjugate Gradients OpenMP 17.0

! Conjugate Gradients OpenMP 18.0

! Conjugate Gradients OpenMP 19.0

! Conjugate Gradients OpenMP 20.0

! Conjugate Gradients OpenMP 21.0

! Conjugate Gradients OpenMP 22.0

! Conjugate Gradients OpenMP 23.0

! Conjugate Gradients OpenMP 24.0

! Conjugate Gradients OpenMP 25.0

! Conjugate Gradients OpenMP 26.0

! Conjugate Gradients OpenMP 27.0

! Conjugate Gradients OpenMP 28.0

! Conjugate Gradients OpenMP 29.0

! Conjugate Gradients OpenMP 30.0

! Conjugate Gradients OpenMP 31.0

! Conjugate Gradients OpenMP 32.0

! Conjugate Gradients OpenMP 33.0

! Conjugate Gradients OpenMP 34.0

! Conjugate Gradients OpenMP 35.0

! Conjugate Gradients OpenMP 36.0

! Conjugate Gradients OpenMP 37.0

! Conjugate Gradients OpenMP 38.0

! Conjugate Gradients OpenMP 39.0

=======

! Conjugate Gradients OpenMP

! Conjugate Gradients OpenMP (with MPI)

! Conjugate Gradients MPI

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with MPI)

! Conjugate Gradients OpenMP (with MPI)

! Conjugate Gradients OpenMP

! Conjugate Gradients

! Conjugate Gradients MPI (with MPI)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI

! Conjugate Gradients OpenMP

! Conjugate Gradients

! Conjugate Gradients MPI

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with MPI)

! Conjugate Gradients OpenMP

! Conjugate Gradients OpenMP (with MPI)

! Conjugate Gradients

! Conjugate Gradients MPI

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with MPI)

! Conjugate Gradients OpenMP

! Conjugate Gradients OpenMP (with MPI)

! Conjugate Gradients

! Conjugate Gradients MPI

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with MPI)

! Conjugate Gradients OpenMP

! Conjugate Gradients OpenMP (with MPI)

! Conjugate Gradients

! Conjugate Gradients MPI

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with MPI)

! Conjugate Gradients OpenMP

! Conjugate Gradients OpenMP (with MPI)

! Conjugate Gradients

! Conjugate Gradients MPI

! Conjugate Gradients MPI (with OpenMP)

! Conjugate Gradients MPI (with OpenMP) (with MPI)

! Conjugate Gradients MPI (with MPI)

!

=======

! Conjugate Gradients OpenMP

! Conjugate Gradients (MPI)

! Conjugate Gradients (MPI/OpenMP)

! Conjugate Gradients (MPI/OpenMP) with GPU

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

! Conjugate Gradients (MPI/OpenMP) with GPU and Intel MKL with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler with Intel Compiler

=======

! Conjugate Gradients OpenMP

! Conjugate Gradients MPI

!

=======

! Conjugate Gradients OpenMP (CG-OMP) algorithm is used to solve the linear system of equations. The CG-OMP algorithm is an iterative method that requires only matrix-vector multiplication and vector addition, subtraction, and scaling operations. The CG-OMP algorithm is a parallel algorithm that can be implemented on shared memory multiprocessors. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved by dividing the matrix into submatrices and solving the submatrices in parallel. The CG-OMP algorithm is implemented using the OpenMP API. The parallelism is achieved

=======

! Conjugate Gradients OpenMP

!

=======

! Conjugate Gradients OpenMP 3.

=======

! Conjugate Gradients OpenMP Im


! Prompt: Conjugate Gradients OpenMP\nsubroutine
Synthesizing 10/10 solutions

=======

subroutine conjugate_gradients (n, nz, a, col, row, p, q, x, z, r, w, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45, a46, a47, a48, a49, a50, a51, a52, a53, a54, a55, a56, a57, a58, a59, a60, a61, a62, a63, a64, a65, a66, a67, a68, a69, a70, a71, a72, a73, a74, a75, a76, a77, a78, a79, a80, a81, a82, a83, a84, a85, a86, a87, a88, a89, a90, a91, a92, a93, a94, a95, a96, a97, a98, a99, a100, a101, a102, a103, a104, a105, a106, a107, a108, a109, a110, a111, a112, a113, a114, a115, a116, a117, a118, a119, a120, a121, a122, a123, a124, a125, a126, a127, a128, a129, a130, a131, a132, a133, a134, a135, a136, a137, a138, a139, a140, a141, a142, a143, a144, a145, a146, a147, a148, a149, a150, a151, a152, a153, a154, a155, a156, a157, a158,

=======

subroutine cg_openmp( n, x, b, tol, itmax, it, flag )
  implicit none
  integer, intent(in) :: n, itmax
  real, intent(in) :: tol, b(n)
  real, intent(out) :: x(n), it, flag
  real, allocatable :: r(:), p(:), q(:), xold(:), r0(:)
  real :: alpha, beta, rho, rho0, res, res0
  integer :: i, j

  allocate( r(n), p(n), q(n), xold(n), r0(n) )

  call mvprod( n, x, r )
  do i = 1, n
    r(i) = b(i) - r(i)
    p(i) = r(i)
  enddo

  rho = dotprod( n, r, r )
  res = sqrt( rho )

  if ( res <= tol ) then
    flag = 0
    it = 0
    return
  endif

  do i = 1, n
    xold(i) = x(i)
  enddo

  do j = 1, itmax

    call mvprod( n, p, q )
    alpha = rho / dotprod( n, p, q )
    do i = 1, n
      x(i) = x(i) + alpha * p(i)
      r(i) = r(i) - alpha * q(i)
    enddo

    rho0 = rho
    rho = dotprod( n, r, r )
    res = sqrt( rho )

    if ( res <= tol ) then
      flag = 0
      it = j
      return
    endif

    beta = rho / rho0
    do i = 1, n
      p(i) = r(i) + beta * p(i)
    enddo

  enddo

  flag = 1
  it = itmax
  do i = 1, n
    x(i) = xold(i)
  enddo

  deallocate( r, p, q, xold, r0 )

end subroutine cg_openmp

! Matrix-vector product OpenMP
subroutine mvprod( n, x, y )
  implicit none
  integer, intent(in) :: n
  real, intent(in)

=======

subroutine cg_openmp(n, x, b, tol, maxiter, iter, resid, a)
  implicit none
  integer, intent(in) :: n, maxiter
  real(kind=8), intent(in) :: tol
  real(kind=8), intent(inout) :: x(n), b(n)
  real(kind=8), intent(out) :: resid
  integer, intent(out) :: iter
  real(kind=8), intent(in) :: a(n,n)
  real(kind=8) :: r(n), p(n), Ap(n)
  real(kind=8) :: rdot, rdotnew, alpha, beta
  integer :: i, j
  !$omp parallel num_threads(4)
  !$omp master
  call cpu_time(t1)
  !$omp end master
  !$omp end parallel
  !$omp parallel num_threads(4)
  !$omp do
  do i = 1, n
    r(i) = b(i)
    p(i) = r(i)
  enddo
  !$omp end do
  !$omp end parallel
  rdot = dot_product(n, r, r)
  do iter = 1, maxiter
    !$omp parallel num_threads(4)
    !$omp do
    do j = 1, n
      Ap(j) = 0.0
      do i = 1, n
        Ap(j) = Ap(j) + a(i,j) * p(i)
      enddo
    enddo
    !$omp end do
    !$omp end parallel
    alpha = rdot / dot_product(n, p, Ap)
    !$omp parallel num_threads(4)
    !$omp do
    do i = 1, n
      x(i) = x(i) + alpha * p(i)
      r(i) = r(i) - alpha * Ap(i)
    enddo
    !$omp end do
    !$omp end parallel
    rdotnew = dot_product(n, r, r)
    if (sqrt(rdotnew) < tol) then
      resid = sqrt(rdotnew)
      !$omp parallel num_threads(4)
      !$omp master
      call cpu_time(t2)
      !$omp end master
      !$omp end parallel
      return
    endif
    beta = rdotnew / rdot
    rdot = rdot

=======

subroutine CG_OpenMP(N, A, x, b, max_iter, tol, iter, residual)
    implicit none
    integer, intent(in) :: N, max_iter
    real, intent(in) :: A(N,N), b(N)
    real, intent(out) :: x(N), residual(max_iter+1)
    real, intent(inout) :: tol
    integer, intent(out) :: iter

    real :: p(N), r(N), Ap(N), alpha, beta, rr, old_rr, sum
    integer :: i, j

    ! Initial guess
    x = 0.0

    ! Initial residual
    r = b - matvec(A, x, N)

    ! Initial search direction
    p = r

    ! Initial residual norm
    old_rr = dot(r, r, N)

    ! Iteration loop
    do i = 1, max_iter
        ! Matrix-vector product
        Ap = matvec(A, p, N)

        ! Step length
        alpha = old_rr / dot(p, Ap, N)

        ! Update solution
        x = x + alpha * p

        ! Update residual
        r = r - alpha * Ap

        ! New residual norm
        rr = dot(r, r, N)

        ! Stopping criterion
        residual(i) = sqrt(rr)
        if (residual(i) < tol) then
            iter = i
            tol = residual(i)
            return
        end if

        ! Update search direction
        beta = rr / old_rr
        p = r + beta * p

        ! Update residual norm
        old_rr = rr
    end do

    iter = max_iter
    tol = residual(max_iter)
end subroutine CG_OpenMP

! Dot product OpenMP
real function dot(x, y, N)
    implicit none
    integer, intent(in) :: N
    real, intent(in) :: x(N), y(N)
    real :: dot

    integer :: i

    dot = 0.0
    !$omp parallel do default(none) shared(x, y, N) reduction(+:dot)
    do i = 1, N
        dot = dot + x(i) * y(i)
    end do
end function dot

! Matrix-vector product OpenMP
real function matvec(A, x, N)
    implicit none

=======

subroutine cg_openmp( n, x, b, r, p, Ap, tol, maxiter, iter, res )
  implicit none
  integer, intent(in) :: n, maxiter
  real, intent(in) :: tol
  real, intent(inout) :: x(n), b(n), r(n), p(n), Ap(n)
  integer, intent(out) :: iter
  real, intent(out) :: res
  real :: alpha, beta, rr, rrnew, rAp
  integer :: i
  ! Initialize the residual
  call matvec( n, x, r )
  do i = 1, n
    r(i) = b(i) - r(i)
  end do
  ! Set the search direction to the residual
  do i = 1, n
    p(i) = r(i)
  end do
  ! Compute the residual norm
  rr = 0.0
  do i = 1, n
    rr = rr + r(i)*r(i)
  end do
  ! Main CG loop
  iter = 0
  do while ( iter < maxiter )
    ! Matrix-vector product
    call matvec( n, p, Ap )
    ! Compute the step length
    rAp = 0.0
    do i = 1, n
      rAp = rAp + r(i)*Ap(i)
    end do
    alpha = rr / rAp
    ! Update the solution approximation
    do i = 1, n
      x(i) = x(i) + alpha*p(i)
    end do
    ! Update the residual
    do i = 1, n
      r(i) = r(i) - alpha*Ap(i)
    end do
    ! Compute the residual norm
    rrnew = 0.0
    do i = 1, n
      rrnew = rrnew + r(i)*r(i)
    end do
    ! Check for convergence
    if ( sqrt(rrnew) < tol ) exit
    ! Compute the new search direction
    beta = rrnew / rr
    do i = 1, n
      p(i) = r(i) + beta*p(i)
    end do
    rr = rrnew
    iter = iter + 1
  end do
  ! Compute

=======

subroutine cg_openmp( n, A, b, x, r, p, Ap, tol, max_iter, iter )
  implicit none
  integer, intent(in) :: n, max_iter
  real, dimension(n,n), intent(in) :: A
  real, dimension(n), intent(in) :: b
  real, dimension(n), intent(inout) :: x
  real, dimension(n), intent(inout) :: r, p, Ap
  real, intent(in) :: tol
  integer, intent(out) :: iter
  real :: alpha, beta
  real :: rr, rr_new
  integer :: i, j

  ! Initialize
  call matvec( n, A, x, r )
  do i = 1, n
    r(i) = b(i) - r(i)
    p(i) = r(i)
  end do

  ! Compute initial residual norm
  rr = 0.0
  do i = 1, n
    rr = rr + r(i)**2
  end do

  ! Main loop
  iter = 0
  do iter = 1, max_iter

    ! Compute Ap = A*p
    call matvec( n, A, p, Ap )

    ! Compute alpha
    alpha = rr
    do i = 1, n
      alpha = alpha - p(i)*Ap(i)
    end do
    alpha = rr / alpha

    ! Update x and r
    do i = 1, n
      x(i) = x(i) + alpha*p(i)
      r(i) = r(i) - alpha*Ap(i)
    end do

    ! Compute new residual norm
    rr_new = 0.0
    do i = 1, n
      rr_new = rr_new + r(i)**2
    end do

    ! Check convergence
    if ( sqrt(rr_new) < tol ) exit

    ! Compute beta
    beta = rr_new / rr

    ! Update p
    do i = 1, n
      p(i) = r(i) + beta*p(i)
    end do

    ! Update rr
    rr = rr_new

  end do

end subroutine cg_openmp

! Matrix-vector multiplication
subroutine matvec( n, A, x, y )
  implicit none

=======

subroutine cg_openmp
  use omp_lib
  implicit none
  integer, parameter :: n = 1000000
  real, parameter :: tol = 1.0e-6
  real, parameter :: rho = 1.0
  real, parameter :: omega = 1.0
  real, parameter :: alpha = 1.0
  real, parameter :: beta = 1.0
  real :: r(n), p(n), x(n)
  real :: a(n), b(n), c(n)
  real :: rr, rr_old, pp, alpha, beta
  integer :: i, it, it_max
  real :: time

  it_max = 1000

  !$omp parallel do
  do i = 1, n
    a(i) = 2.0
    b(i) = -1.0
    c(i) = -1.0
    x(i) = 0.0
  end do

  a(1) = 1.0
  a(n) = 1.0
  c(n-1) = 0.0

  !$omp parallel do
  do i = 1, n
    r(i) = -b(i)*x(i-1) - c(i)*x(i+1) + a(i)*x(i)
  end do

  !$omp parallel do
  do i = 1, n
    p(i) = r(i)
  end do

  rr = 0.0
  !$omp parallel do reduction(+:rr)
  do i = 1, n
    rr = rr + r(i)*r(i)
  end do

  it = 0

  time = omp_get_wtime()

  do while (sqrt(rr) > tol .and. it < it_max)
    it = it + 1

    pp = 0.0
    !$omp parallel do reduction(+:pp)
    do i = 1, n
      pp = pp + p(i)*p(i)
    end do

    alpha = rr / pp

    !$omp parallel do
    do i = 1, n
      x(i) = x(i) + alpha*p(i)
    end do

    !$omp parallel do
    do i = 1, n
      r(i) = r

=======

subroutine cg_openmp (n, x, b, tol, maxit, iter, resid, a, p, r, z, q)
   implicit none
   integer, intent(in) :: n, maxit
   real, intent(in) :: tol
   real, intent(inout) :: x(n)
   real, intent(in) :: b(n), a(n,n), r(n), z(n), q(n), p(n)
   real :: alpha, beta, rho, rho1, resid
   integer :: iter, i, j

   ! Initialize
   call matvec(a, x, r)
   do i = 1, n
      r(i) = b(i) - r(i)
      z(i) = r(i)
   enddo

   rho = dot_product(r, z)

   do iter = 1, maxit

      ! Compute q = A*p
      call matvec(a, p, q)

      alpha = rho / dot_product(p, q)

      ! Update x
      do i = 1, n
         x(i) = x(i) + alpha * p(i)
      enddo

      ! Update r
      do i = 1, n
         r(i) = r(i) - alpha * q(i)
      enddo

      resid = sqrt(dot_product(r, r))
      if (resid .lt. tol) exit

      ! Update z
      call matvec(a, r, z)

      rho1 = dot_product(r, z)

      beta = rho1 / rho

      rho = rho1

      ! Update p
      do i = 1, n
         p(i) = z(i) + beta * p(i)
      enddo

   enddo

end subroutine cg_openmp

! Matrix-vector product
subroutine matvec(a, x, y)
   implicit none
   real, intent(in) :: a(*,*), x(*)
   real, intent(out) :: y(*)

   integer :: n, i, j

   n = size(a, dim=1)

   do i = 1, n
      y(i) = 0.0
      do j = 1, n
         y(i) = y(i) + a(i,j) * x(j)
      enddo
   enddo

end subroutine matvec

! Matrix-vector product

=======

subroutine cg_openmp(A, x, b, max_iter, tol, n, rank, num_threads)
  implicit none
  real, intent(in) :: A(:,:), x(:), b(:)
  real, intent(out) :: x(:)
  integer, intent(in) :: max_iter, n, rank, num_threads
  real, intent(in) :: tol
  real :: r(n), p(n), Ap(n)
  real :: alpha, beta, rTr, rTr_old, rTr_new
  integer :: i, j, iter
  !$omp parallel num_threads(num_threads) private(i,j)
  !$omp master
  do i = 1, n
    r(i) = b(i)
    do j = 1, n
      r(i) = r(i) - A(i, j) * x(j)
    end do
    p(i) = r(i)
  end do
  !$omp end master
  !$omp barrier
  !$omp master
  rTr = 0.0
  do i = 1, n
    rTr = rTr + r(i) * r(i)
  end do
  !$omp end master
  !$omp barrier
  !$omp master
  iter = 0
  !$omp end master
  !$omp barrier
  !$omp master
  do while (iter < max_iter)
    !$omp end master
    !$omp barrier
    !$omp master
    do i = 1, n
      Ap(i) = 0.0
      do j = 1, n
        Ap(i) = Ap(i) + A(i, j) * p(j)
      end do
    end do
    !$omp end master
    !$omp barrier
    !$omp master
    rTr_old = rTr
    rTr = 0.0
    do i = 1, n
      rTr = rTr + r(i) * r(i)
    end do
    !$omp end master
    !$omp barrier
    !$omp master
    alpha = rTr / (p %*% Ap)
    do i = 1, n
      x(i) = x(i) + alpha * p(i)
      r(i) = r(i) - alpha * Ap(i)
    end do
    rTr_new = 0

=======

subroutine cg_openmp( A, b, x, n, tol, maxit, niter, res, time )
  implicit none
  real*8, intent(in) :: A(n,n), b(n)
  real*8, intent(inout) :: x(n)
  integer, intent(in) :: n, maxit, niter
  real*8, intent(in) :: tol
  real*8, intent(out) :: res, time
  real*8 :: r(n), p(n), z(n), Ap(n)
  real*8 :: rho, rho1, alpha, beta
  real*8 :: t1, t2
  integer :: i, j

  call cpu_time( t1 )

  call dcopy( n, b, 1, r, 1 )
  call dgemv( 'N', n, n, -1.0d0, A, n, x, 1, 1.0d0, r, 1 )
  call dcopy( n, r, 1, p, 1 )
  rho = ddot( n, r, 1, r, 1 )

  do i = 1, maxit
    call dgemv( 'N', n, n, 1.0d0, A, n, p, 1, 0.0d0, Ap, 1 )
    alpha = rho / ddot( n, p, 1, Ap, 1 )
    call daxpy( n, alpha, p, 1, x, 1 )
    call daxpy( n, -alpha, Ap, 1, r, 1 )
    rho1 = ddot( n, r, 1, r, 1 )
    if ( sqrt(rho1) < tol ) then
      niter = i
      res = sqrt(rho1)
      exit
    endif
    beta = rho1 / rho
    call dscal( n, beta, p, 1 )
    call daxpy( n, 1.0d0, r, 1, p, 1 )
    rho = rho1
  enddo

  call cpu_time( t2 )
  time = t2 - t1

end subroutine cg_openmp

! Conjugate Gradients OpenMP with MPI

! Prompt: Conjugate Gradients OpenACC\nsubroutine
Synthesizing 9/10 solutions

=======

subroutine conjugate_gradients( n, a, b, x )
  implicit none
  integer, intent(in) :: n
  real, dimension(n,n), intent(in) :: a
  real, dimension(n), intent(in) :: b
  real, dimension(n), intent(inout) :: x
  integer :: i, j, k
  real :: r( n ), p( n ), rr( n ), alpha, beta
  real :: sum

  !$acc enter data copyin(a,b,x)
  !$acc enter data create(r,p,rr)

  !$acc parallel loop
  do i = 1, n
    r(i) = -b(i)
    do j = 1, n
      r(i) = r(i) + a(i,j) * x(j)
    enddo
    p(i) = r(i)
  enddo

  !$acc parallel loop
  do i = 1, n
    rr(i) = r(i) * r(i)
  enddo

  do k = 1, 50

    !$acc parallel loop
    do i = 1, n
      sum = 0.
      do j = 1, n
        sum = sum + a(i,j) * p(j)
      enddo
      r(i) = sum
    enddo

    alpha = 0.
    !$acc parallel loop reduction(+:alpha)
    do i = 1, n
      alpha = alpha + rr(i) / ( p(i) * r(i) )
    enddo

    !$acc parallel loop
    do i = 1, n
      x(i) = x(i) + alpha * p(i)
      r(i) = r(i) - alpha * r(i)
    enddo

    beta = 0.
    !$acc parallel loop reduction(+:beta)
    do i = 1, n
      beta = beta + r(i) * r(i)
    enddo

    if ( sqrt(beta) < 1.0E-10 ) exit

    beta = beta / alpha

    !$acc parallel loop
    do i = 1, n
      p(i) = r(i) + beta * p(i)
    enddo

  enddo

  !$acc exit data delete(r,p,rr)
  !$acc exit data copyout(x)

end

=======

subroutine cg_openacc (A, b, x, n, max_iter, tol)
    real, intent(in) :: A(n,n), b(n)
    real, intent(inout) :: x(n)
    integer, intent(in) :: n, max_iter
    real, intent(in) :: tol

    integer :: i, iter
    real :: alpha, beta, rtr, rtr_new, rtr_old, rtr0
    real :: r(n), p(n), Ap(n)

    !$acc data copyin(A,b), copy(x), create(r,p,Ap)
    !$acc kernels
    do i = 1, n
        r(i) = b(i)
        p(i) = r(i)
    end do
    !$acc end kernels

    rtr = dot_product(r, r, n)

    do iter = 1, max_iter
        !$acc kernels
        do i = 1, n
            Ap(i) = 0.0
        end do
        !$acc end kernels

        !$acc parallel loop
        do i = 1, n
            Ap(i) = Ap(i) + A(i,i) * p(i)
        end do

        alpha = rtr / dot_product(p, Ap, n)

        !$acc kernels
        do i = 1, n
            x(i) = x(i) + alpha * p(i)
            r(i) = r(i) - alpha * Ap(i)
        end do
        !$acc end kernels

        rtr_new = dot_product(r, r, n)

        beta = rtr_new / rtr

        !$acc kernels
        do i = 1, n
            p(i) = r(i) + beta * p(i)
        end do
        !$acc end kernels

        rtr = rtr_new

        if (sqrt(rtr) < tol) exit

    end do

    !$acc exit data delete(r,p,Ap)
end subroutine cg_openacc

! Conjugate Gradients OpenMP
subroutine cg_openmp (A, b, x, n, max_iter, tol)
    real, intent(in) :: A(n,n), b(n)
    real, intent(inout) :: x(n)
    integer, intent(in) :: n, max_iter
    real, intent(in) :: tol

    integer :: i

=======

subroutine cg_openacc(n, a, x, b, r, p, q, z, rho, rho1, alpha, beta)
  integer, intent(in) :: n
  real, intent(in) :: a(n,n), b(n)
  real, intent(inout) :: x(n), r(n), p(n), q(n), z(n)
  real, intent(inout) :: rho, rho1, alpha, beta
  integer :: i, j

  !$acc kernels
  do i = 1, n
    r(i) = 0.0
    do j = 1, n
      r(i) = r(i) + a(i,j) * x(j)
    end do
    r(i) = b(i) - r(i)
    p(i) = r(i)
  end do
  !$acc end kernels

  rho = 0.0
  do i = 1, n
    rho = rho + r(i) * r(i)
  end do

  do i = 1, n
    q(i) = 0.0
    do j = 1, n
      q(i) = q(i) + a(i,j) * p(j)
    end do
  end do

  alpha = rho / (p(1:n) * q(1:n))

  do i = 1, n
    x(i) = x(i) + alpha * p(i)
    r(i) = r(i) - alpha * q(i)
  end do

  rho1 = 0.0
  do i = 1, n
    rho1 = rho1 + r(i) * r(i)
  end do

  beta = rho1 / rho

  do i = 1, n
    p(i) = r(i) + beta * p(i)
  end do

  rho = rho1

end subroutine cg_openacc

! Conjugate Gradients OpenMP
subroutine cg_openmp(n, a, x, b, r, p, q, z, rho, rho1, alpha, beta)
  integer, intent(in) :: n
  real, intent(in) :: a(n,n), b(n)
  real, intent(inout) :: x(n), r(n), p(n), q(n), z(n)
  real, intent(inout

=======

subroutine cg_openacc( n, x, b, tol, maxit, iter, resid )
implicit none
integer, intent(in) :: n, maxit
real, intent(in) :: tol
real, intent(inout) :: x(n)
real, intent(in) :: b(n)
integer, intent(out) :: iter
real, intent(out) :: resid
real :: r(n), p(n), Ap(n)
real :: alpha, beta, rr, rrnew
integer :: i
!$acc data copyin(b(1:n)), copy(x(1:n))
!$acc kernels
do i = 1, n
r(i) = b(i)
p(i) = r(i)
enddo
!$acc end kernels
rr = dot_product( n, r, r )
iter = 0
do while ( rr > tol*tol .and. iter < maxit )
iter = iter + 1
call matvec( n, p, Ap )
alpha = rr / dot_product( n, p, Ap )
!$acc kernels
do i = 1, n
x(i) = x(i) + alpha * p(i)
r(i) = r(i) - alpha * Ap(i)
enddo
!$acc end kernels
rrnew = dot_product( n, r, r )
beta = rrnew / rr
rr = rrnew
!$acc kernels
do i = 1, n
p(i) = r(i) + beta * p(i)
enddo
!$acc end kernels
enddo
resid = sqrt( rr )
!$acc end data
end subroutine cg_openacc

! Conjugate Gradients OpenMP
subroutine cg_openmp( n, x, b, tol, maxit, iter, resid )
implicit none
integer, intent(in) :: n, maxit
real, intent(in) :: tol
real, intent(inout) :: x(n)
real, intent(in) :: b(n)
integer, intent(out) :: iter
real, intent(out) :: resid
real :: r(n), p(n), Ap(n)
real :: alpha, beta, rr, rrnew
integer :: i
!$omp parallel default(none) shared(n, x, b) private(i)
!$omp do
do i = 1, n
r(i) = b(i)
p(i)

=======

subroutine conjugate_gradients( n, a, r, p, q, x, z, rro, rho, sum, i, j )
  integer, intent(in) :: n
  real, intent(in) :: a(n,n)
  real, intent(inout) :: r(n), p(n), q(n), x(n), z(n)
  real, intent(inout) :: rro, rho, sum
  integer, intent(inout) :: i, j

  !$acc kernels
  do j = 1, n
    q(j) = 0.0
    do i = 1, n
      q(j) = q(j) + a(i,j) * p(i)
    enddo
  enddo

  sum = 0.0
  do i = 1, n
    sum = sum + p(i) * q(i)
  enddo

  rho = rro / sum

  do i = 1, n
    x(i) = x(i) + rho * p(i)
    r(i) = r(i) - rho * q(i)
  enddo

  rro = 0.0
  do i = 1, n
    rro = rro + r(i) * r(i)
  enddo
  !$acc end kernels

end subroutine conjugate_gradients

! Conjugate Gradients OpenACC
subroutine conjugate_gradients( n, a, r, p, q, x, z, rro, rho, sum, i, j )
  integer, intent(in) :: n
  real, intent(in) :: a(n,n)
  real, intent(inout) :: r(n), p(n), q(n), x(n), z(n)
  real, intent(inout) :: rro, rho, sum
  integer, intent(inout) :: i, j

  !$acc kernels
  do j = 1, n
    q(j) = 0.0
    do i = 1, n
      q(j) = q(j) + a(i,j) * p(i)
    enddo
  enddo

  sum = 0.0
  do i = 1, n
    sum = sum + p(i) * q(i)
  enddo

  rho = rro / sum

  do i =

=======

subroutine cg_openacc( a, x, b, n, max_iter, tol, iter, residual )
    implicit none
    integer, intent(in) :: n, max_iter
    real, intent(in) :: tol
    real, intent(inout) :: x(n)
    real, intent(in) :: a(n,n), b(n)
    real, intent(out) :: residual
    integer :: iter
    real :: r(n), p(n), Ap(n), alpha, beta, rr, rrold
    integer :: i, j

    !$acc data copyin(a(1:n,1:n),b(1:n)) copyout(x(1:n)) create(r(1:n),p(1:n),Ap(1:n))
    !$acc kernels
    do i = 1, n
        x(i) = 0.0
        r(i) = b(i)
        p(i) = r(i)
    enddo
    !$acc end kernels
    rr = dot_product( r, r, n )
    if ( rr < tol*tol ) then
        residual = sqrt(rr)
        iter = 0
        return
    endif
    do iter = 1, max_iter
        !$acc kernels
        do i = 1, n
            Ap(i) = 0.0
            do j = 1, n
                Ap(i) = Ap(i) + a(i,j) * p(j)
            enddo
        enddo
        !$acc end kernels
        rrold = rr
        alpha = rr / dot_product( p, Ap, n )
        !$acc kernels
        do i = 1, n
            x(i) = x(i) + alpha * p(i)
            r(i) = r(i) - alpha * Ap(i)
        enddo
        !$acc end kernels
        rr = dot_product( r, r, n )
        if ( rr < tol*tol ) then
            residual = sqrt(rr)
            return
        endif
        beta = rr / rrold
        !$acc kernels
        do i = 1, n
            p(i) = r(i) + beta * p(i)
        enddo
        !$acc end kernels
    enddo
    residual = sqrt(rr)
    !$acc end data
end subroutine cg_openacc

! Con

=======

subroutine cg_openacc(n, A, x, b, tol, maxiter, iter, minres)
   use omp_lib
   implicit none
   integer, intent(in) :: n
   real, intent(in) :: A(n,n)
   real, intent(inout) :: x(n)
   real, intent(in) :: b(n)
   real, intent(in) :: tol
   integer, intent(in) :: maxiter
   integer, intent(out) :: iter
   integer, intent(out) :: minres
   integer :: i, j, k, m
   real :: alpha, beta, rho, rho1, rho2, tmp
   real, dimension(n) :: p, q, r, s, t
   real :: start_time, end_time
   real :: dtime

   start_time = omp_get_wtime()
   minres = 0
   iter = 0

   !$acc data copyin(A(1:n,1:n), x(1:n), b(1:n)) copyout(r(1:n), p(1:n), q(1:n), s(1:n), t(1:n))
   !$acc kernels
   do i = 1, n
      r(i) = b(i)
      do j = 1, n
         r(i) = r(i) - A(i,j)*x(j)
      enddo
      p(i) = r(i)
   enddo
   !$acc end kernels
   rho1 = 0.0
   do i = 1, n
      rho1 = rho1 + r(i)*r(i)
   enddo

   do k = 1, maxiter
      iter = k
      !$acc kernels
      do i = 1, n
         q(i) = 0.0
         do j = 1, n
            q(i) = q(i) + A(i,j)*p(j)
         enddo
      enddo
      !$acc end kernels
      rho2 = 0.0
      do i = 1, n
         rho2 = rho2 + p(i)*q(i)
      enddo
      alpha = rho1/rho2
      do i = 1, n
         s(i) = x(i) + alpha*p(i)
         t(i) = r(i) - alpha

=======

subroutine cg_openacc (b, x, n, tol, max_iter, iter, r, p, Ap)
    implicit none
    real (kind=8), dimension(:), intent(in) :: b
    real (kind=8), dimension(:), intent(inout) :: x
    integer, intent(in) :: n, max_iter
    real (kind=8), intent(out) :: tol
    integer, intent(out) :: iter
    real (kind=8), dimension(:), intent(inout) :: r, p, Ap
    real (kind=8) :: alpha, beta, rnew, rold, r0, r1, normb
    integer :: i, j
    !$acc enter data copyin(b) copyin(x) copyin(r) copyin(p) copyin(Ap)
    !$acc enter data copyout(tol) copyout(iter)
    r0 = 0.0d0
    r1 = 0.0d0
    do i = 1, n
        r(i) = b(i)
        p(i) = r(i)
        r0 = r0 + r(i) * r(i)
    end do
    normb = sqrt(r0)
    if (normb == 0.0d0) normb = 1.0d0
    do iter = 1, max_iter
        !$acc parallel loop present(Ap, p)
        do i = 1, n
            Ap(i) = 0.0d0
            do j = 1, n
                Ap(i) = Ap(i) + A(i, j) * p(j)
            end do
        end do
        rnew = 0.0d0
        alpha = 0.0d0
        do i = 1, n
            alpha = alpha + r(i) * r(i)
        end do
        alpha = alpha / (p %*% Ap)
        do i = 1, n
            x(i) = x(i) + alpha * p(i)
            r(i) = r(i) - alpha * Ap(i)
            rnew = rnew + r(i) * r(i)
        end do
        if (sqrt(rnew / normb) < tol) then
            tol = sqrt(rnew / normb)

=======

subroutine cg( n, x, b, r, p, q, cg_iter, cg_eps )
use oacc_kernels, only: cg_kernel
implicit none
integer, parameter :: max_iter = 100
real, parameter :: eps = 1.0e-8
integer :: i, iter, n
real :: alpha, beta, bnorm, rho, rho_1, rnorm
real, dimension(n) :: x, b, r, p, q
integer :: cg_iter
real :: cg_eps
! Initialize
call oacc_kernels::cg_kernel( n, x, b, r, p, q )
bnorm = sqrt( dot_product( b, b ) )
rho = dot_product( r, r )
rnorm = sqrt( rho )
! Main loop
do iter = 1, max_iter
if ( rnorm/bnorm <= eps ) exit
rho_1 = rho
rho = dot_product( r, r )
beta = rho / rho_1
p = r + beta * p
call oacc_kernels::cg_kernel( n, x, b, r, p, q )
rho = dot_product( r, r )
rnorm = sqrt( rho )
end do
cg_iter = iter
cg_eps = rnorm/bnorm
end subroutine cg

! Conjugate Gradients OpenACC kernel
module oacc_kernels
contains
subroutine cg_kernel( n, x, b, r, p, q )
implicit none
integer :: i, n
real :: alpha, beta, bnorm, rho, rho_1, rnorm
real, dimension(n) :: x, b, r, p, q
! Main loop
do i = 1, n
q(i) = dot_product( p, A(:,i) )
end do
alpha = rho / dot_product( p, q )
x = x + alpha * p
r = r - alpha * q
end subroutine cg_kernel
end module oacc_kernels

The first step is to create the OpenACC kernels module. This is a simple module with a single subroutine. The subroutine is called cg_kernel and it contains the main loop of the conjugate gradient algorithm.

! Conjugate Gradients OpenACC kernel
module oacc_kernels
contains
subroutine cg_kernel( n, x, b, r, p, q )
implicit none

! Prompt: Conjugate Gradients OpenMP offload\nsubroutine
Synthesizing 10/10 solutions

=======

subroutine cg_openmp_offload( a, x, b, n )
  implicit none
  real, dimension(:,:), intent(in) :: a
  real, dimension(:), intent(inout) :: x
  real, dimension(:), intent(in) :: b
  integer, intent(in) :: n
  integer :: i, j, k
  real, dimension(n) :: r, p, Ap
  real, dimension(n) :: xold
  real :: alpha, beta, rr, rrnew
  real :: rnorm, bnorm, r0norm, eps, t1, t2
  real :: r0, r1, r2, r3, r4, r5, r6, r7, r8, r9
  real :: r10, r11, r12, r13, r14, r15, r16, r17, r18, r19
  real :: r20, r21, r22, r23, r24, r25, r26, r27, r28, r29
  real :: r30, r31, r32, r33, r34, r35, r36, r37, r38, r39
  real :: r40, r41, r42, r43, r44, r45, r46, r47, r48, r49
  real :: r50, r51, r52, r53, r54, r55, r56, r57, r58, r59
  real :: r60, r61, r62, r63, r64, r65, r66, r67, r68, r69
  real :: r70, r71, r72, r73, r74, r75, r76, r77, r78, r79
  real :: r80, r81, r82, r83, r84, r85, r86, r87, r88, r89
  real :: r90, r91, r92, r93, r94, r95, r96, r97, r98, r99
  real :: p0, p1, p2, p3, p4, p5, p6, p7, p8, p9
  real :: p10, p11

=======

subroutine conjugate_gradients(n, a, r, p, q, x, s, z, tmp)
  implicit none
  integer, intent(in) :: n
  real(wp), intent(in) :: a(n,n)
  real(wp), intent(in) :: r(n)
  real(wp), intent(inout) :: p(n), q(n), x(n)
  real(wp), intent(inout) :: s, z, tmp
  integer :: i, j
  !$omp target teams distribute parallel do map(tofrom: x) map(to: a, r) map(tofrom: p, q, s, z, tmp)
  do j = 1, n
    x(j) = 0.0_wp
    p(j) = r(j)
    q(j) = 0.0_wp
  end do
  s = 0.0_wp
  do i = 1, n
    s = s + r(i) * r(i)
  end do
  !$omp target teams distribute parallel do map(tofrom: x) map(to: a, r) map(tofrom: p, q, s, z, tmp)
  do j = 1, n
    do i = 1, n
      q(j) = q(j) + a(i,j) * p(i)
    end do
  end do
  z = 0.0_wp
  !$omp target teams distribute parallel do reduction(+: z) map(tofrom: x) map(to: a, r) map(tofrom: p, q, s, z, tmp)
  do i = 1, n
    z = z + p(i) * q(i)
  end do
  tmp = s / z
  !$omp target teams distribute parallel do map(tofrom: x) map(to: a, r) map(tofrom: p, q, s, z, tmp)
  do i = 1, n
    x(i) = x(i) + tmp * p(i)
    r(i) = r(i) - tmp * q(i)
  end do
  s = 0.0_wp
  !$omp target teams distribute parallel do reduction(+: s) map(tofrom: x) map(to: a, r) map(tofrom: p, q, s, z, tmp)
  do i = 1,

=======

subroutine cg_openmp_offload(A, b, x, max_iter, tol, iter, res_norm)
  implicit none
  integer, intent(in) :: max_iter
  real, intent(in) :: tol
  real, intent(in) :: A(:,:), b(:), x(:)
  integer, intent(out) :: iter
  real, intent(out) :: res_norm
  integer :: n
  real :: r(:), p(:), Ap(:), alpha, beta, r_r, r_r_new, b_b
  integer :: i
  real, dimension(:), allocatable :: w
  integer :: omp_is_initialised

  n = size(b)
  call omp_get_initial_device(omp_is_initialised)
  if (omp_is_initialised == 0) then
    call omp_set_default_device(0)
    call omp_set_num_threads(4)
  end if
  allocate(w(n))

  r = b - matvec(A, x)
  p = r
  r_r = dot_product(r, r)
  b_b = dot_product(b, b)
  do i = 1, max_iter
    Ap = matvec(A, p)
    alpha = r_r / dot_product(p, Ap)
    x = x + alpha * p
    r = r - alpha * Ap
    r_r_new = dot_product(r, r)
    if (sqrt(r_r_new / b_b) < tol) exit
    beta = r_r_new / r_r
    p = r + beta * p
    r_r = r_r_new
  end do
  iter = i
  res_norm = sqrt(r_r / b_b)

  deallocate(w)
end subroutine cg_openmp_offload

! Conjugate Gradients OpenMP offload with tasking
subroutine cg_openmp_offload_tasking(A, b, x, max_iter, tol, iter, res_norm)
  implicit none
  integer, intent(in) :: max_iter
  real, intent(in) :: tol
  real, intent(in) :: A(:,:), b(:), x(:)
  integer, intent(out) :: iter
  real, intent(out) :: res_norm
  integer :: n
  real :: r(:), p(:), Ap(:), alpha, beta, r_r, r_r_new, b_b
  integer :: i
  real,

=======

subroutine cg_openmp_offload(n, x, b, tol, max_iter, iter, err)
  use omp_lib
  implicit none
  integer, intent(in) :: n, max_iter
  real, dimension(n), intent(out) :: x
  real, dimension(n), intent(in) :: b
  real, intent(in) :: tol
  integer, intent(out) :: iter
  real, intent(out) :: err

  integer :: i, j, k
  real, dimension(n) :: r, p, Ap
  real :: rsold, rsnew, alpha, beta
  real :: t1, t2, t3

  !$omp target data map(to: b(1:n)) map(tofrom: x(1:n))
  !$omp target teams distribute parallel for
  do i = 1, n
    x(i) = 0.0
  enddo

  !$omp target teams distribute parallel for
  do i = 1, n
    r(i) = b(i)
    p(i) = r(i)
  enddo

  rsold = 0.0
  !$omp target teams distribute parallel for reduction(+:rsold)
  do i = 1, n
    rsold = rsold + r(i)*r(i)
  enddo

  do k = 1, max_iter
    !$omp target teams distribute parallel for
    do i = 1, n
      Ap(i) = 0.0
    enddo

    !$omp target teams distribute parallel for collapse(2)
    do i = 1, n
      do j = 1, n
        Ap(i) = Ap(i) + A(i,j)*p(j)
      enddo
    enddo

    alpha = 0.0
    !$omp target teams distribute parallel for reduction(+:alpha)
    do i = 1, n
      alpha = alpha + p(i)*Ap(i)
    enddo
    alpha = rsold/alpha

    !$omp target teams distribute parallel for
    do i = 1, n
      x(i) = x(i) + alpha*p(i)
    enddo

    !$omp target teams distribute parallel for
    do i = 1, n
      r(i) = r(i) - alpha*Ap(i)
    enddo

=======

subroutine cg_openmp_offload( n, a, b, x, tol, max_iter, iter, err )
  use omp_lib
  implicit none
  integer, intent(in) :: n, max_iter
  real, intent(in) :: a(n,n), b(n), tol
  real, intent(out) :: x(n), err
  integer :: iter
  real :: p(n), r(n), ap(n)
  real :: alpha, beta, rr, old_rr
  real :: tmp
  integer :: i, j, k

  do i = 1, n
    x(i) = 0.0
  end do

  call omp_set_num_threads( 4 )

  !$omp target enter data map(to: a(1:n,1:n), b(1:n))
  !$omp target enter data map(alloc: x(1:n), p(1:n), r(1:n), ap(1:n))

  !$omp target teams distribute parallel do
  do i = 1, n
    p(i) = 0.0
    r(i) = b(i)
  end do

  rr = 0.0
  !$omp target teams distribute parallel do reduction(+:rr)
  do i = 1, n
    rr = rr + r(i) * r(i)
  end do

  iter = 0
  do
    iter = iter + 1
    if ( iter > max_iter ) exit

    !$omp target teams distribute parallel do
    do i = 1, n
      ap(i) = 0.0
    end do

    !$omp target teams distribute parallel do collapse(2) private(tmp)
    do k = 1, n
      do j = 1, n
        tmp = a(k,j)
        ap(k) = ap(k) + tmp * p(j)
      end do
    end do

    alpha = rr / (p,ap)

    !$omp target teams distribute parallel do
    do i = 1, n
      x(i) = x(i) + alpha * p(i)
      r(i) = r(i) - alpha * ap(i)
    end do

    old_rr = rr
    rr = 0.0
    !$omp target teams distribute parallel do reduction(+:rr)
    do i = 1

=======

subroutine cg_openmp_offload( n, A, b, x, tol, maxit, nit, err )
  integer, intent(in) :: n, maxit
  real, intent(in) :: A(n,n), b(n), tol
  real, intent(inout) :: x(n)
  integer, intent(out) :: nit
  real, intent(out) :: err
  real :: r(n), p(n), Ap(n)
  real :: alpha, beta, rr, rrnew
  integer :: i, j

  !$omp target enter data map(to: A(1:n,1:n), b(1:n)) &
  !$omp& map(alloc: x(1:n), r(1:n), p(1:n), Ap(1:n))

  !$omp target teams distribute parallel do
  do i = 1, n
    x(i) = 0.0
  enddo

  !$omp target teams distribute parallel do
  do i = 1, n
    r(i) = b(i)
    p(i) = r(i)
  enddo

  !$omp target teams distribute parallel do reduction(+:rr)
  do i = 1, n
    rr = rr + r(i)*r(i)
  enddo

  do i = 1, maxit
    !$omp target teams distribute parallel do
    do j = 1, n
      Ap(j) = 0.0
    enddo

    !$omp target teams distribute parallel do
    do j = 1, n
      do i = 1, n
        Ap(j) = Ap(j) + A(i,j)*p(i)
      enddo
    enddo

    alpha = rr

    !$omp target teams distribute parallel do reduction(+:alpha)
    do j = 1, n
      alpha = alpha - p(j)*Ap(j)
    enddo

    alpha = rr/alpha

    !$omp target teams distribute parallel do
    do j = 1, n
      x(j) = x(j) + alpha*p(j)
    enddo

    !$omp target teams distribute parallel do
    do j = 1, n
      r(j) = r(j) - alpha*Ap(j)
    enddo

    rrnew = 0.0

    !$omp target teams distribute parallel do reduction

=======

subroutine cg_openmp_offload (A, b, x, tol, max_iter, n, nnz)
    implicit none
    integer, intent(in) :: n, nnz, max_iter
    real, intent(in) :: tol
    real, intent(in) :: A(nnz, 3), b(n)
    real, intent(inout) :: x(n)
    real :: r(n), p(n), Ap(n), alpha, beta, r_new, r_old
    integer :: iter, i
    !$omp target data map(to: A(1:nnz, 1:3), b(1:n)) map(tofrom: x(1:n))
    !$omp target teams distribute parallel for
    do i = 1, n
        x(i) = 0.0
    end do
    !$omp end target data
    call matvec(A, x, Ap, n, nnz)
    !$omp target data map(to: Ap(1:n)) map(tofrom: r(1:n))
    !$omp target teams distribute parallel for
    do i = 1, n
        r(i) = b(i) - Ap(i)
    end do
    !$omp end target data
    r_new = dot(r, r, n)
    p = r
    iter = 0
    do
        iter = iter + 1
        call matvec(A, p, Ap, n, nnz)
        alpha = r_new / dot(p, Ap, n)
        !$omp target teams distribute parallel for
        do i = 1, n
            x(i) = x(i) + alpha * p(i)
        end do
        !$omp end target teams
        !$omp target teams distribute parallel for
        do i = 1, n
            r(i) = r(i) - alpha * Ap(i)
        end do
        !$omp end target teams
        r_old = r_new
        r_new = dot(r, r, n)
        if (sqrt(r_new) < tol) exit
        beta = r_new / r_old
        !$omp target teams distribute parallel for
        do i = 1, n
            p(i) = r(i) + beta * p(i)
        end do
        !$omp end target teams
    end do
end subroutine cg_openmp_offload

!

=======

subroutine cg_openmp_offload( a, x, b, r, p, z, q, n, niter, tol )
  implicit none
  real, dimension(1:n,1:n), intent(in) :: a
  real, dimension(1:n), intent(in) :: b
  real, dimension(1:n), intent(inout) :: x
  real, dimension(1:n), intent(inout) :: r
  real, dimension(1:n), intent(inout) :: p
  real, dimension(1:n), intent(inout) :: z
  real, dimension(1:n), intent(inout) :: q
  integer, intent(in) :: n
  integer, intent(in) :: niter
  real, intent(in) :: tol
  real :: rho, rho1, alpha, beta
  integer :: i, j, k
  real :: sum

  !$omp target data map(to:a,b) map(tofrom:x,r,p,z,q)
  !$omp target teams distribute parallel for
  do j = 1, n
    x(j) = 0.0
  enddo

  !$omp target teams distribute parallel for
  do j = 1, n
    r(j) = b(j)
  enddo

  !$omp target teams distribute parallel for
  do j = 1, n
    p(j) = r(j)
  enddo

  !$omp target teams distribute parallel for reduction(+:sum)
  do j = 1, n
    sum = 0.0
    do i = 1, n
      sum = sum + a(j,i) * p(i)
    enddo
    z(j) = sum
  enddo

  !$omp target teams distribute parallel for reduction(+:sum)
  do j = 1, n
    sum = 0.0
    do i = 1, n
      sum = sum + a(i,j) * z(i)
    enddo
    q(j) = sum
  enddo

  !$omp target teams distribute parallel for reduction(+:rho)
  do j = 1, n
    rho = rho + r(j) * z(j)
  enddo

  do k = 1, niter
    alpha = rho / q
    !$omp target teams distribute parallel for

=======

subroutine cg_openmp_offload( n, x, b, tol, maxit, nb, A, xexact, niter, rnorm )
  implicit none
  integer, intent(in) :: n, nb, maxit
  real(8), intent(in) :: tol
  real(8), intent(in) :: xexact(n)
  real(8), intent(inout) :: x(n), b(n)
  real(8), intent(in) :: A(n,n)
  integer, intent(out) :: niter
  real(8), intent(out) :: rnorm
  real(8) :: r(n), p(n), Ap(n)
  real(8) :: alpha, beta, rr, rrnew
  integer :: i, j, k
  ! OpenMP offload
  real(8) :: rrnew_local(omp_get_max_threads())
  real(8) :: rr_local(omp_get_max_threads())
  ! initialize
  call matvec( n, A, x, r )
  do i = 1, n
    r(i) = b(i) - r(i)
  end do
  do i = 1, n
    p(i) = r(i)
  end do
  rr = 0.0d0
  do i = 1, n
    rr = rr + r(i) * r(i)
  end do
  ! main loop
  do k = 1, maxit
    call matvec( n, A, p, Ap )
    alpha = rr / dot(n, p, Ap)
    do i = 1, n
      x(i) = x(i) + alpha * p(i)
    end do
    do i = 1, n
      r(i) = r(i) - alpha * Ap(i)
    end do
    rrnew = 0.0d0
    !$omp parallel private(i,j) shared(n,r,rnorm,rr,rrnew,rrnew_local,rr_local)
    rrnew_local(omp_get_thread_num()) = 0.0d0
    !$omp do
    do i = 1, n
      rrnew_local(omp_get_thread_num()) = rrnew_local(omp_get_thread_num()) + r(i) * r(i)
    end do
    !$omp end do
    !$omp

=======

subroutine conjugate_gradients( n, a, x, z, p, q, r, w, & 
  & cg_iterations, cg_residual, cg_time, cg_normr, cg_normr0 )
  implicit none
  integer, intent(in) :: n
  real, intent(in) :: a(n,n)
  real, intent(in) :: x(n)
  real, intent(out) :: z(n)
  real, intent(inout) :: p(n), q(n), r(n), w(n)
  integer, intent(out) :: cg_iterations
  real, intent(out) :: cg_residual
  real, intent(out) :: cg_time
  real, intent(out) :: cg_normr, cg_normr0

  real :: d, sum, rho, rho0, alpha, beta
  integer :: k
  real :: t1, t2

  call omp_set_num_threads( threads )

  ! Initialize the CG algorithm
  call omp_target_data map(to: a(1:n,1:n), x(1:n)) &
                       map(tofrom: p(1:n), q(1:n), r(1:n), w(1:n))
  call omp_target_enter_data map(to: a(1:n,1:n), x(1:n)) &
                             map(tofrom: p(1:n), q(1:n), r(1:n), w(1:n))
  call omp_target_update map(to: a(1:n,1:n), x(1:n)) &
                         map(tofrom: p(1:n), q(1:n), r(1:n), w(1:n))

  call omp_target teams distribute parallel do
  do k=1,n
    p(k) = 0.0
    q(k) = 0.0
    r(k) = x(k)
    w(k) = 0.0
  end do

  call omp_target teams distribute parallel do reduction(+:sum)
  do k=1,n
    sum = 0.0
    do j=1,n
      sum = sum + a(k,j) * r(j)
    end do
    w(k) = sum
  end do

  call omp_target teams distribute parallel do reduction(+:sum)
  do k=1,n
    sum = 0.0
    do j=1,n
      sum = sum
